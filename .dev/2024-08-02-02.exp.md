# Overview of Parameters

change setWriteCellsTool -> setColorRedTool because gpt-4o tended to use an incompatible encoding of red (hex instead of "red"). Changed the tool because nowhere was the model informed of what encoding to use. I need to tell it what encoding to use.

# Directional Change

Basically garbage change. In 20-30 runs, gpt-4o didn't succeed a single time.

# Numbers

```
{
  "numExperiments": 20,
  "numSuccessful": 0,
  "numFailures": 20,
  "averageProgress": 33.9,
  "averageFailureProgress": 33.9,
  "averageNumMessages": 38.45,
  "averageSuccessMessages": null,
  "averageFailureMessages": 38.45,
  "averageToolCalls": 17.95,
  "averageFailureToolCalls": 17.95,
  "averageSuccessToolCalls": null,
  "model": "gpt-4o"
}
```

# Failure Modes

Two failure modes, one is I think new (i.e. the mini model doesn't do it):

- early exit, thinking it's done when it's not
- falseNegative failure loop (this is the new one)

Basically what's happening in the new one is that it keeps trying to set the same cell, saying that it's not working, but the cell was in fact set correctly.

It seems like the model has really poor resolution and can't target a single cell. It may also be a problem with zero indexing but I do tell the model things are zero indexed, and I do include grid lines, and I do include results of tool calls that show zero-indexed results. I'm sure there's a better way to facilitate getting the indexing right.

Including better diffs might help with this.

# Notes

One thing I noticed is that gpt-4o seems to execute multiple calls in one request more often.

# Next

- try bigger and smaller grids
- include an explicit diff that says progress (to avoid early exits)
- try non-vision models