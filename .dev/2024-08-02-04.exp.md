# Overview of Parameters

This experiment is basically the same as the previous but I added
some error tracking and stall detection. I don't think stall detection will hurt performance because the LLMs tend to get really stuck when things start to repeat, they just keep repeating on and on. So in any case where they make no progress for 10 iterations they almost certainly would not have made it out anyways.

# Directional Change

Still really bad performance. 5% success.

# Numbers

```
{
  "numExperiments": 10,
  "numSuccessful": 1,
  "numFailures": 9,
  "averageProgress": 64.1,
  "averageFailureProgress": 60.111111111111114,
  "averageNumMessages": 116.4,
  "averageSuccessMessages": 120,
  "averageFailureMessages": 116,
  "averageToolCalls": 74.6,
  "averageFailureToolCalls": 71.33333333333333,
  "averageSuccessToolCalls": 104,
  "model": "gpt-4o-mini"
}
```

# Failure Modes

So we have

- false positive early exits
- invalid call parameters
- progress stalling
- random non-function call responses

# Notes

The only success on this one was _NOT_ all at once, but it was big chunks (30-then40,then overshooting). It's interesting that it did not correctly detect it's completion, it kept going after it got to 100.

_Soon I should see what happens if I encourage large chunk tool calls._

# Next

I'm going to add in a better diff and see if it gets better.

