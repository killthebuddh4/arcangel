# Overview of Parameters

I added a stall loop annealer thing. Basically the model gets stuck in this loop where it makes the same tool call over and over and over again. I added a thing that detects progress stalls and then tells the model to do something differently.


# Directional Change

The success rate went from 7 to 3, with 5 stalls. So I'd call this pretty neutral. The annealer seems to have done nothing.


# Numbers

```
{
  "numExperiments": 10,
  "numSuccessful": 3,
  "numFailures": 7,
  "averageProgress": 74.4,
  "averageFailureProgress": 63.42857142857143,
  "averageNumMessages": 156.2,
  "averageSuccessMessages": 78,
  "averageFailureMessages": 189.71428571428572,
  "averageToolCalls": 111.9,
  "averageFailureToolCalls": 132.42857142857142,
  "averageSuccessToolCalls": 64,
  "model": "gpt-4o-mini"
}
```

# Failure Modes

Stalling basically.

# Notes

All 3 successes followed the same pattern:

- 1 square in the first response
- 64 squares in the second response

It seems like the successes tend to be generally big batches of tool calls.

I wonder what a batched tool would do (instead of setColorCell it's setColorCell__s__)?

# Next

I think I should try a few more things on the annealer with basic prompt engineering. I want to do as little as possible to keep the methods as general as possible.

I think I'm going to give a concrete example in the annealer. This won't translate to problems where I don't have the answer already though, so not really what I want. But we will see.



