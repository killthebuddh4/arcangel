# So where are we at.

I've toyed around with getting gpt-4o to both generate a rule and to generate
grids, without a ton of success.

One thing that's interesting is that the models can _kind of do a good job at
describing input grids_ but are bad at coming up with generation rules and
really really bad at generating grids naively.

The important test I ran was to see how well the model could just literally
generate the grid for a given image. So you give it an image of a grid and a
mapping between colors, and the model gives you the encoded version of that
grid. The models could not even a little bit generate the encodings accurately,
which seems pretty bad for the chances that we can generate grids correctly.

So one shot grid generation is no good, so I tried a kind of naive loop where I
have the parsers return error strings and pass those back to the models. It
didn't seem like the model was really using the errors to improve, and ended up
just generating a bunch of semi-independent images.

The ideas above and a couple other considerations led to my decision to abandon
the project...

kidding.

my decision to create a kind of workshop/shell for the LLLms to operate on
grids. The LLM will send commands to the shell and the shell will respond with
relevant state snippets. Once we give the LLM tools for working with grids then
we'll loop back to the "generate a copy of the input" problem and see what
happens.



