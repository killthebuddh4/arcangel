#

So this is the first result set, so I'm still kinda figuring out what I want to do.

- General overview of the relevant parameters that were changed
- General directional overview of the changes
  - Did the changes lead to an improvement? How much? Any specific notes on this?
  - Did the changes lead to a regression? How much? Any specific notes on this?
- Numeric summary of the run
  - number of experiments
  - number of successes
  - number of failures
  - average progress
  - average progress for failure
  - average num messages
  - average num messages for failure
  - average num messages for success
  - model name
- A high-level description of the failure modes.

__TODO Instead of just numbers, a bar chart with experiment ids as the independent would ge good.__

# Overview of Parameters

This was the first experiment so I didn't change anything. The relevant knobs, or at least the lowest hanging ones, are, I think:

- the system prompt
- providing examples
- returning encoded representations of workinggrid
- showing working grid as image
- size of grid (bigger probably harder?)
- we didn't show a diff
- the LLM

I think I'm going to give these knobs codes, so I can reference them:

-  SYSTEM_PROMPT
-  EXAMPLES
-  ENCODED_FEEDBACK
-  PROGRESS_FEEDBACK
-  IMAGE_FEEDBACK
-  GRID_SIZE
-  DIFF_FEEDBACK
-  MODEL (vision or not may be an additional parameter)

# Directional Change

I didn't record the results, but a purely naive approach with no examples and no feedback was very, very ineffective. Like 0% success.

# Numbers

```
{
  "numExperiments": 43,
  "numSuccessful": 29,
  "numFailures": 14,
  "averageProgress": 88.02325581395348,
  "averageFailureProgress": 63.214285714285715,
  "averageNumMessages": 48.906976744186046,
  "averageSuccessMessages": 52.689655172413794,
  "averageFailureMessages": 41.07142857142857,
  "averageToolCalls": 14.465116279069768,
  "averageFailureToolCalls": 11.142857142857142,
  "averageSuccessToolCalls": 16.06896551724138,
  "model": "gpt-4o-mini"
}
```

# Failure Modes

The most common failure mode is just the model thinking it's done when it's
only partially done. It seems like this becomes an issue at around 70% progress.
I wonder if it would be better for big grids because they're easier to see.

Another failure (it only happened once) was the model instead of continuing with tool calls decided to ask whether to continue.

Another failure mode is the model said it thought it was just repeating the same tool calls over and over and not making progress, so it quit. 

# Next

- try bigger and smaller grids
- include an explicit diff that says progress (to avoid early exits)
- try non-vision models
- try gpt-4o