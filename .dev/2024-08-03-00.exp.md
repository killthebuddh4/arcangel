# Overview of Parameters

Basically just ran a bunch more of the previous exp.


# Directional Change

Success rate 88%, so slightly down, but I'd be surprised if the true value is less than, say 85.
# Numbers

```
{
  "numExperiments": 50,
  "numSuccessful": 44,
  "numFailures": 6,
  "averageProgress": 93.18,
  "averageFailureProgress": 43.166666666666664,
  "averageNumMessages": 133.26,
  "averageSuccessMessages": 132.52272727272728,
  "averageFailureMessages": 138.66666666666666,
  "averageToolCalls": 85.62,
  "averageFailureToolCalls": 40.5,
  "averageSuccessToolCalls": 91.77272727272727,
  "model": "gpt-4o-mini"
}
```

# Failure Modes

- 4 random text responses
- 2 parse failures (one might be a bug in my code)


# Notes

- There was one run with 745 tool calls that eventually succeeded!!!
- The 0-1-64 pattern is becoming more common? Def seems the most common success.
- Tool calls tend to fit a patter (one row at a time, one column at a time, 2x2 grid by 2x2 grid).
- When the filled-out shape becomes slightly less round (i.e. not the result of some big shape write) the model seems significantly more likely to get bogged down and heavily rely on the annealer.

I wonder if I can get the model to articulate its strategy, it will get less bogged down.

Also, I may have already said this but I want to try giving examples that help the model see better. Like show examples of detailed writes with feedback that says what happened.

# Next

- use a less-powerful annealer
- use better examples (maybe without the annealer)



